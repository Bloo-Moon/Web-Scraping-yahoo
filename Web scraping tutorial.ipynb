{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27663d13-6d4d-4bcc-be7f-462807667f87",
   "metadata": {},
   "source": [
    "## Web scraping \n",
    "\n",
    "The workflow of web scraping not only includes getting data online but also includes the process of turning the data into something readable and usable since in most cases, the data scraped are unstructered.  \n",
    "The steps of web scraping are:  \n",
    "- Locate the URL for which you want to scrape data from;\n",
    "- Inspect the webpage to identify the \"Tags\"/\"Path\"/\"Selector\"/\"Attributes\" of the content you want to scrape;\n",
    "- Write the code and make sure your code works;\n",
    "- Generalize your codes to scrape more webpages, but make sure to not let yourself be blocked. _there are tips in later sections_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ee210-33e7-4ea2-86ac-ca8c62b75d89",
   "metadata": {},
   "source": [
    "## Yahoo Finance News\n",
    "\n",
    "This tutorial will follow the above steps to extract news articles and related information from Yahoo Finance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a486a43-69e5-465c-9937-81222bfcfbbc",
   "metadata": {},
   "source": [
    "### 1. Understand HTTP Requests and Responses\n",
    "its useful to understand how information is transmitted between servers and clients before we start web scraping. Hyper Text Transfer Protocol (HTTP) is a messaging protocol that connects a client with the server.  \n",
    "The communication occurs in a request-response cycle between a client and the server replies with a **_response_**.  \n",
    "The most common requests a client can send is the `GET` request and the `POST` request.  \n",
    "GET request is a client asks the server to send information back, while POST request is asking a client to send information to the server, thus it is more widely used in web applications.   \n",
    "For web scraping, we usually send a GET request and wait for the server to respond.   \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb0678-e619-4892-9adb-a3cd4a022c3c",
   "metadata": {},
   "source": [
    "### 2. Scrape information from one webpage\n",
    "How do we scape information from a webpage? We need to get a GET request to the server and extract the information we want from the response.  \n",
    "Specifically here are the steps:\n",
    "- Send the request \n",
    "- Parse the HTML response\n",
    "- Locate and extract the information we need \n",
    "\n",
    "#### 2.1 Send the request \n",
    "I need to get 2 pieces of information to scrape a website. One is the URL of the webpage, another one is the page source.  \n",
    "For chrome you can inspect the page either by looking at the developer's tool or by right clicking the mouse and choosing \"inspect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445e2a6-68b4-4425-bf74-7db3a1883011",
   "metadata": {},
   "source": [
    "<img src = 'page_source.JPG' width=800 length=800/>\n",
    "\n",
    "The code on the top right panel are the codes for us to explore and locate the information we care about.  \n",
    "It is the respose that the server sends back from a get request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdca2f1b-a2af-4b18-8d9e-9e5cb48dbf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 'OK')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://news.yahoo.com/dems-head-toward-house-control-160107166.html'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.status_code,response.reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf8a708-f878-4c2f-a2f3-601a06f1f21e",
   "metadata": {},
   "source": [
    "There are five categories of status code showing different situations during this step:  \n",
    "\n",
    "<img src = 'categories.png' width=600 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f66276f-02f1-4891-a9f2-238fe15902f8",
   "metadata": {},
   "source": [
    "#### 2.2 Parse the response \n",
    "\n",
    "After getting the response from the server, we need to parse the raw HTML and make it readable to python, so that we can extract valuable information from it.  \n",
    "The main parsers are `BeautifulSoup` and `lxml`. `lxml` is much faster but is terrible at dealing with multiformed HTML, thus we usually use `BeautifulSoup` to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0a2244-b2f0-4fac-8a77-9a7ef8921bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817532d-2eba-4208-a9e4-36f64733c7c5",
   "metadata": {},
   "source": [
    "The soup variable will carry all the information from the webpage, thus it is long and tedious.  \n",
    "The next step is to extract the data we need from soup. To do that we need to go back to the webpage and examine it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00688d-1a58-4989-bb65-ae443851bd48",
   "metadata": {},
   "source": [
    "#### 2.3 Locate and extract the data we need \n",
    "First we know we want tp extract the news article and the timestamp for when the news was released.  \n",
    "It is important because we need to match the label variable, depending on what you're looking for, using the timestamp. Lets go back to the webpage and find them.  \n",
    "\n",
    "The trick I always use is that I first locate what is the information I want to extract from the webpage, in this case I want to get the news body.  \n",
    "I then select parts of the news and click 'inspect'. This way instead of showing the beginning of the HTML code, it is highlighting what I have selected on the right top panel. \n",
    "\n",
    "<img src = 'paragraph.JPG' width = 800 height = 800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f0f5a-e8b4-4f68-a579-bbcfefd850a1",
   "metadata": {},
   "source": [
    "Taking a closer look:  \n",
    "\n",
    "<img src = 'closer.png' width = 600 height = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c7b38-0c52-4784-9be6-5f66000bd2e5",
   "metadata": {},
   "source": [
    "Its showing that the news contents are stored under under the tags 'p'. What we need to do next is to grab these pieces of information from soup. There are several ways to extract the information after knowing where they are.  \n",
    "One way is to use the tag name and its attribution, another way is to use the CSS select.  \n",
    "For this case, I will use the tag name with the functions **`find`** from BeautifulSoup since it is more straightforward. \n",
    "\n",
    "<img src = 'find.png' width = 800 height = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a66060-d6ec-4824-9824-6d7f5b763e58",
   "metadata": {},
   "source": [
    "Now our goal is to extract all the paragraphs from the news, not just one. Looking at the HTML, we can see that all the paragraphs are in each tag names 'p', and under a division, 'div'.  \n",
    "The most convenient was of doing this is to find the division and extract all the paragraph at once: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a6611c-e0ea-4c2d-9f12-8324ca4a0247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"caas-body\"><p>WASHINGTON (AP) — Disappointed Democrats headed Wednesday toward renewing their control of the House for two more years but with a potentially shrunken majority as they lost at least seven incumbents without ousting a single Republican lawmaker.</p><p>By Wednesday afternoon, Democrats' only gains were two North Carolina seats vacated by GOP incumbents after a court-ordered remapping made the districts more Democratic. Although their majority seemed secure, the results were an unexpected jolt for a party that had envisioned gains of perhaps 15 seats. They were a morale booster for Republicans, who going into Election Day were mostly bracing for losses.</p><p>“They were all wrong,\" House Minority Leader Kevin McCarthy, R-Calif., told reporters about Democrats' assumptions of adding to their House numbers. Repeating a campaign theme Republicans used repeatedly against Democrats, he said, “The rejection that we saw last night from the Democrats, was that America does not want to be a socialist nation.″</p><p>McCarthy also touted his party's modest additions to its small cadre of female and minority lawmakers. “The Republican coalition is bigger, more diverse and more energetic than ever before,\" he said.</p><p>In perhaps their highest profile triumph, Republicans finally defeated 15-term Rep. Collin Peterson from a rural Minnesota district that backed President Donald Trump in 2016 by 31 percentage points, Trump's biggest margin in any Democratic-held district. Peterson, who chairs the House Agriculture Committee, is one of the House's most conservative Democrats but was defeated by Republican Michelle Fischbach, the former lieutenant governor.</p><p>The latest Democratic incumbent to fall was freshman Rep. Abby Finkenauer of northeastern Iowa, who lost to GOP state Rep. Ashley Hinson.</p><p>The other defeated Democrats — all freshmen — included Reps. Debbie Mucarsel-Powell and Donna Shalala, health secretary under President Bill Clinton, in adjacent South Florida districts.</p><div class=\"caas-readmore caas-readmore-collapse\"><button aria-label=\"\" class=\"link rapid-noclick-resp caas-button collapse-button\" data-ylk=\"elm:readmore;slk:Story continues\" title=\"\">Story continues</button></div><p>Joe Cunningham of South Carolina, Xochitl Torres Small of New Mexico and Kendra Horn in Oklahoma also lost. All had won surprising 2018 victories in districts Trump carried decisively in 2016.</p><p>The fight for Torres Small's seat cost around $35 million, making it one of the country's most expensive races, according to the nonpartisan Center for Responsive Politics. She was defeated by Yvette Herrell, a former state legislator.</p><p>Before votes were counted, both parties’ operatives said the GOP would be fortunate to limit Democratic gains to modest single digits. Democrats control the House 232-197, with five open seats and one independent. It takes 218 seats to control the chamber.</p><p>Democrats were also disappointed in the <a class=\"link rapid-noclick-resp\" data-ylk=\"slk:Senate,\" href=\"https://apnews.com/article/control-of-senate-at-stake-election-4255da17a16505fb9ee20ae64b2153b3\" rel=\"nofollow noopener\" target=\"_blank\">Senate,</a> where they nursed fading hopes of winning the majority. <a class=\"link rapid-noclick-resp\" data-ylk=\"slk:Trump's challenge from Democrat Joe Biden\" href=\"https://apnews.com/article/donald-trump-joe-biden-election-day-966a3decc2c3262946c03baf2dd14e8f\" rel=\"nofollow noopener\" target=\"_blank\">Trump's challenge from Democrat Joe Biden</a> remained too early to call.</p><p>A smaller Democratic majority would make it tougher for House Speaker Nancy Pelosi, D-Calif., to unite her lawmakers as a handful of progressive freshmen arrive for the new Congress.</p><p>By retaining House control, Democrats would mark only the second time in a quarter century that they've led the chamber for two consecutive two-year Congresses. The first period ran from 2007 through 2010, when Pelosi was serving her first four years as speaker.</p><p>“Our purpose in this race was to win so that we could protect the Affordable Care Act and so that we could crush the virus,” Pelosi told reporters, citing former President Barack Obama’s health care act. She declared that Democrats had won the House majority, which seemed highly likely but hadn’t been officially determined by The Associated Press.</p><p>Democrats’ hopes of protecting their majority and even expanding it were based on public anxiety over the pandemic, Trump’s alienation of suburban voters, and a vast fundraising edge. But those advantages didn't carry them as far as they'd hoped.</p><p>Democrats' setbacks were measured not just by seats they lost but by districts they failed to capture.</p><p>Democrats lost a majority Hispanic district in West Texas they expected to win after the GOP incumbent retired. And they lost a series of what seemed coin-flip races, failing to defeat GOP incumbents in Cincinnati, rural Illinois, central Virginia and the suburbs of St. Louis and several districts in Texas.</p><p>In a district between Austin and San Antonio, freshman GOP Rep. Chip Roy withstood a challenge from Democrat Wendy Davis. Davis gained fame as a state legislator by waging a 2013 filibuster against an anti-abortion bill, then lost a race for governor the following year. The conservative Club for Growth made her its biggest target, spending over $6 million against her this year.</p><p>As if symbolically, Illinois Rep. Cheri Bustos, who leads the Democratic Congressional Campaign Committee, was in her own tight race in a closely divided district she won by 24 percentage points in 2018.</p><p>Democrats also notched no victories in long-shot races they’d hoped would bolster their majority. Republicans retained such seats in central North Carolina; Montana; Omaha, Nebraska; and around Little Rock, Arkansas.</p><p>Some endangered Democratic freshmen like Texas' Lizzie Fletcher, Georgia's Lucy McBath and New Jersey's Tom Malinowski and Andy Kim held on. And as Wednesday progressed, other hotly fought races remained undecided in Indiana and Virginia.</p><p>Democratic Rep. Alexandria Ocasio-Cortez of New York and the three other members of the so-called squad of young progressive women of color were easily reelected. A small handful of new progressives will be coming to Washington including Democrats Jamaal Bowman and Mondaire Jones of New York, but others like Nebraska Democrat Kara Eastman lost their races.</p><p>Jones will be Congress' first openly gay black lawmaker, while another newcomer from New York, Democrat Ritchie Torres, will be its first openly gay Hispanic.</p><p>Marjorie Taylor Greene, who has espoused unfounded QAnon conspiracy theories, won a vacant seat in northwest Georgia. Trump has called Greene a “future Republican star.” QAnon asserts that Trump is quietly waging a battle against pedophiles in government.</p><p>Republican Lauren Boebert, a guns-rights activist and bar owner from western Colorado, won an open GOP seat there.</p><p>Hanging over the contests were the coronavirus pandemic and the wounded economy, which voters ranked as top concerns, according to AP VoteCast, a national survey of the electorate. The virus has killed 232,000 people in the U.S. and cases are rising in nearly every state, while millions have lost jobs.</p><p>Democrats thought Trump’s repeated false statements downplaying the virus’ severity would redouble the impact of their long-time focus on health care.</p><p>Buoying Democrats was a coast-to-coast edge in campaign fundraising. That allowed nearly all Democratic incumbents in potentially vulnerable districts to outspend their GOP challengers, often by vast margins, according to an AP analysis of Federal Election Commission campaign reports.</p><p>AP VoteCast is a nationwide survey of more than 127,000 voters and nonvoters conducted for The Associated Press by NORC at the University of Chicago.</p><p>___</p><p>Find AP’s full election coverage at APNews.com/Election2020.</p></div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', attrs={'class': 'caas-body'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587b618-1392-43de-9bb0-97ae54ba230d",
   "metadata": {},
   "source": [
    "The find function will find the first tag that satisfies the condition specified in the function. We use the class attribution and specify its name to be the condition.  \n",
    "We can see that soup has found us the right information, but we need to remove the HTML format, just keep the news article as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f075d9d8-0248-47cb-a95f-c250cc18cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = soup.find('div', attrs={'class': 'caas-body'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a649620-c603-4dfe-9537-7dca9e6e1bdf",
   "metadata": {},
   "source": [
    "We can see that the \"news\" variable has captured all the news body, with some extra symbols like '\\'. However, it doesn't matter since we usually delete symbols when pre-processing text data.  \n",
    "\n",
    "There is another function from BeautifulSoup called `find_all`. instead of returning the first tag, **find_all** will return all tags that satisfy the condition as a list. This can be helpful when extracting all information from a common tag, like all links from a webpage.  \n",
    "For the news paragraphs, if we use **find_all**, we need to add an extra step of joining all list elements together into one string, which is not as convenient as using **find**.  \n",
    "\n",
    "We can also get the headlines of the news. Following the same procedure, we find the headline is here:\n",
    "\n",
    "<img src = 'headline.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd29da-de43-40e8-bfa0-810ff7bb5bb5",
   "metadata": {},
   "source": [
    "the headline is in a unique tag _**h1**_ in the beginning. It is very convenient to extract with the find function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afeb1d91-a90e-4401-a671-58b8a9df4815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 data-test-locator=\"headline\">Dems head toward House control, but GOP picks off seats</h1>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed27f49-cf6d-4016-9d1a-e20ce2d1cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's extra information in there mostly specifying the style, we can extract the text only:\n",
    "headline = soup.find('h1').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986088f-7d9a-4f76-9c32-07ac5ddd1307",
   "metadata": {},
   "source": [
    "we also want to know what date this news was published, following the same process:  \n",
    "<img src = \"date.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84cadfd-8754-40c7-859e-a5d19eb2f805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'November 4, 2020, 6:01 PM'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = soup.find('time').text\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad54702-1766-43a5-b836-0a2996115d80",
   "metadata": {},
   "source": [
    "Unlike the news articles, we do not want the date to be a string variable. Thus, we need to convert the string into a timestamp as a date. The function we can use is `strptime` from `datetime`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa6d6e5a-580c-47d2-81a3-264d5cb9dca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 11, 4, 6, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of using the code above, we can add strptime here:\n",
    "from datetime import datetime \n",
    "\n",
    "date = datetime.strptime(soup.find('time').text, \"%B %d, %Y, %H:%M %p\")\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61721e-9ad9-4530-adde-c75e9f8cb930",
   "metadata": {},
   "source": [
    "the function `strptime()` takes in two inputs, the string and the format. The string is the date we scraped and based on how the string looks like, we need to define a pattern for **strptime()** to recognize it.  \n",
    "According to the reference %B is the full word for the month, %Y is the complete digits for the year, %H:%M is the time in hours and minutes and %p is AM/PM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7d1bfe-a604-4bd0-b688-72ada44a5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info():\n",
    "    '''\n",
    "    Now that we have all the information we need, we can write a function to format the code\n",
    "    '''\n",
    "    #send request \n",
    "    response = requests.get(url)\n",
    "    #parse\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    #get information you need\n",
    "    news = soup.find('div', attrs={'class': 'caas-body'}).text\n",
    "    headline = soup.find('h1').text\n",
    "    date = datetime.strptime(soup.find('time')).text, \"%B %d, %Y %H:%M %p\"\n",
    "    \n",
    "    return news, headline, date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b2cbb-f8df-4f7a-a587-dc9c39d61d9a",
   "metadata": {},
   "source": [
    "### 3. Fetching subsequent pages\n",
    "so far we have scraped one news article, we then need to scrape other news articles. As long as the websites are in a similar format, and we know their URLs we can use the same function we defined in the previous section to extract the news body, headline, and date one by one.  \n",
    "The only information we are missing here is the URLs of a list of news articles. We can get the list of URLs by web scraping a webpage with a list of news articles:\n",
    "[https://finance.yahoo.com/topic/stock-market-news]\n",
    "\n",
    "#### 3.1 Get the links\n",
    "This website has a lot of news articles listed, we need to locate the external link to each news articles and store them in a list. Links are usually located in a tag named by **'a'** with the attribute name **'href'**:\n",
    "\n",
    "<img src = 'links.png' width = 800 height = 800>\n",
    "\n",
    "for this website, it provides a lot of list tags **'li'** to store each news article's information. The links are in the *'a'* tag, called *'href'*. We can use the **find_all** function to extract all the links inside the list tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66cff2d7-bd13-42f5-b9b0-07c52ad8130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://finance.yahoo.com/topic/stock-market-news\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "link = [l.find('a')['href'] for l in soup.find_all('li') if l.find('a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38386fd2-36fc-4f11-bcd8-55eadf66f451",
   "metadata": {},
   "source": [
    "However, using this code will make the parser find all the links inside the ‘li’ tags, including links we do not desire, like links to advertisements. In cases like this, when the patterns are nested deep inside a common tag, there are two special convenience languages for such traversals: `CSS selectors` and `XPath`. They are both ways to identify the specific locations for the information we desire. I will use CSS selectors here.  \n",
    "\n",
    "We want to only get links from _‘li’_s for news articles, each _‘li’_ for a news article should have a unique CSS selector that leads us there. On the webpage HTML panel, the easiest way to get the specific CSS selector is to select any li tag, right click, and choose ‘copy selector’. For the first news article, its CSS selector is _“#Fin-Stream > ul > li:nth-child(1)”_. We want to go directly to the ‘a’ tag of this ‘li’ tag, so I write _“#Fin-Stream > ul > li:nth-child(1) a”_ instead. The select function will return a list, to extract the attribute ‘href’ from the ‘a’ tag, we need to first slice the list and specify the attribute name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7327af23-d29d-4098-b5c6-f2ab3bdb3ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/m/013f3a26-06d7-3bb6-8811-ddbc901f9eec/these-are-the-best-robinhood.html'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('#Fin-Stream > ul > li:nth-child(1) a')[0]['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e6ca9-a108-4e10-a993-6c8b9559e1c8",
   "metadata": {},
   "source": [
    "We have successfully extracted the link for the first news article, but compare to the actual link of the news article,\n",
    "\n",
    "“https://finance.yahoo.com/news/election-2020-stock-market-news-updates-november-5-2020-232052919.html”,  \n",
    "\n",
    "we are missing “https://finance.yahoo.com”. We can add this part by extending the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89384ef2-5e48-439e-abd2-a3cc745e02cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://finance.yahoo.com/m/013f3a26-06d7-3bb6-8811-ddbc901f9eec/these-are-the-best-robinhood.html'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'https://finance.yahoo.com' + soup.select('#Fin-Stream > ul > li:nth-child(1) a')[0]['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6884d0-5bc9-459c-b8b1-a8bca78a7f90",
   "metadata": {},
   "source": [
    "We can also notice that the CSS selector has ‘1’ inside, indicating the first article. We can manipulate the CSS selector to extract more links. Using list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86fffb83-658e-4c26-a4a2-076fa094be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://finance.yahoo.com' + soup.select('#Fin-Stream > ul > li:nth-child({}) a'.format(i))[0]['href'] for i in range(1,4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2136f0-2adb-49ec-a751-acfd4ed88c69",
   "metadata": {},
   "source": [
    "the variable 'links' contains the first 3 news articles on this webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "671ae49f-b00f-42a4-b854-e7dd6ad2dd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://finance.yahoo.com/m/013f3a26-06d7-3bb6-8811-ddbc901f9eec/these-are-the-best-robinhood.html',\n",
       " 'https://finance.yahoo.com/m/0aaba121-6543-313e-bb26-dc7e76f84a36/one-800flowers-com-stock.html',\n",
       " 'https://finance.yahoo.com/m/a49abab5-9171-3baa-8141-e76126c6aaa3/penn-national-gaming-stock.html']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d20b03-f006-41f7-9f89-fdd24b3070df",
   "metadata": {},
   "source": [
    "#### 3.2 Extract information for each link\n",
    "After storing the links into a list we can then scrape news information for multiple news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62d58288-2174-4bdb-b5f0-33fd5c4b7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(url):\n",
    "    #grab the information\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    news = soup.find('div', attrs = {'class': 'caas-body'}).text\n",
    "    headline = soup.find('h1').text\n",
    "    date = datetime.strptime(soup.find('time').text, \"%B %d, %Y, %I:%M %p\")\n",
    "    \n",
    "    #combine all info into a list of columns \n",
    "    columns = [news, headline, date]\n",
    "    \n",
    "    #give columns names \n",
    "    column_names = ['News', 'Headline', 'Date']\n",
    "    \n",
    "    return dict(zip(column_names, columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb450d-a315-4488-951e-d106aa382eb2",
   "metadata": {},
   "source": [
    "Note that the news published date for these websites also includes the hour, minute, am/pm, so we need to change the fitting format in *strptime*. We also need to think about how to store the information for a list of news. You can store them as a list of tuples, a dictionary, or a DataFrame, depending on the next step. The code I provided above will store the data for each news as a dictionary. By list comprehension, we can store a list of news as a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6cf6b57-d1b8-41fa-8813-b2b604c52d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [get_info(url) for url in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39791b3e-2d49-48c5-b05a-48eb60f0e54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buying a stock is easy, but purchasing the rig...</td>\n",
       "      <td>These Are The Best Robinhood Stocks To Buy Or ...</td>\n",
       "      <td>2021-06-09 00:05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When considering what names to put on your wat...</td>\n",
       "      <td>One-800Flowers.com Stock Clears Key Benchmark,...</td>\n",
       "      <td>2021-06-08 23:46:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Relative Strength Rating upgrade for Penn Na...</td>\n",
       "      <td>Penn National Gaming Stock Flashes Improved Re...</td>\n",
       "      <td>2021-06-08 23:26:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  \\\n",
       "0  Buying a stock is easy, but purchasing the rig...   \n",
       "1  When considering what names to put on your wat...   \n",
       "2  A Relative Strength Rating upgrade for Penn Na...   \n",
       "\n",
       "                                            Headline                Date  \n",
       "0  These Are The Best Robinhood Stocks To Buy Or ... 2021-06-09 00:05:00  \n",
       "1  One-800Flowers.com Stock Clears Key Benchmark,... 2021-06-08 23:46:00  \n",
       "2  Penn National Gaming Stock Flashes Improved Re... 2021-06-08 23:26:00  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which we can turn into a dataframe \n",
    "import pandas as pd\n",
    "pd.DataFrame(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a88d7-656f-4f79-ae5d-60b9491c718d",
   "metadata": {},
   "source": [
    "#### 3.3 More links\n",
    "Normally, this method would work to extract as many links as this webpage has. However, one challenge I am facing here is that this website that stores all the links for the news is a **dynamic website**. This means that the server will only send the top three news articles back to the request, so we cannot extract over three links using the method above. For some websites, they only display limited information per page. We can then manipulate the page parameter in the URL to scrape all URLs for a bunch of pages. For example, Yelp only displays ten restaurants for each page, we can scrape all links for the ten restaurants in page 1, then scrape page 2, and so on.  \n",
    "\n",
    "One solution for this is to use Selenium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "592ba39b-9378-4ab7-9873-7e72a55b5968",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c3803ada6b62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#define webdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\chromedriver'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#use your own path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[0;32m     79\u001b[0m                     \u001b[0mremote_server_addr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     keep_alive=keep_alive),\n\u001b[1;32m---> 81\u001b[1;33m                 desired_capabilities=desired_capabilities)\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\u001b[0m\n\u001b[0;32m    155\u001b[0m             warnings.warn(\"Please use FirefoxOptions to set browser profile\",\n\u001b[0;32m    156\u001b[0m                           DeprecationWarning, stacklevel=2)\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrowser_profile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSwitchTo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mobile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMobile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[1;34m(self, capabilities, browser_profile)\u001b[0m\n\u001b[0;32m    250\u001b[0m         parameters = {\"capabilities\": w3c_caps,\n\u001b[0;32m    251\u001b[0m                       \"desiredCapabilities\": capabilities}\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'sessionId'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             return self.request_encode_body(\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             )\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    675\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m             )\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    424\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1354\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tatenda bwerinofa\\appdata\\local\\programs\\python\\python37\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "url = 'https://finance.yahoo.com/topic/stock-market-news'\n",
    "\n",
    "links = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    \n",
    "    #define webdriver\n",
    "    driver = webdriver.Chrome('C:\\chromedriver') #use your own path\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    #get request\n",
    "    driver.get(url)\n",
    "    \n",
    "    #keep clicking each news article and grab their url \n",
    "    elem = driver.find_element_by_css_selector(\"#Fin-Stream > ul > li:nth-child({})\".format(i))\n",
    "    elem.click()\n",
    "    \n",
    "    #store in a list\n",
    "    links.append(driver.current_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3524e8d-b3a1-46cb-87e1-9edded70d207",
   "metadata": {},
   "source": [
    "The solution I used here should not be the best one to deal with this issue since it takes a very long time, and it clicks the webpage all the time.  \n",
    "\n",
    "### 4. How to reduce the chances of getting blocked\n",
    "Before web scraping, it is essential to check whether the website allows being scraped. You can search the web scraping rule for the website, or check related information on the website.  \n",
    "You are always facing a chance of being blocked by the server during web scraping even though the website is web scraping friendly. Most of the time, it is because you are sending the request too freaquently and too many times in a short period of time. There are several methods you can adopt to reduce the changes of being blocked.  \n",
    "\n",
    "#### 4.1 Specify a header\n",
    "When sending the request, you can specify headers in the request function by using the website's `Request Headers`. You can find the request headers in the same panel that contains HTML information, but in the **Network** tab:\n",
    "\n",
    "<img src = 'request_header.png' width = 800 height = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043d041-acc3-4fd5-8cde-5a5c2712d62a",
   "metadata": {},
   "source": [
    "Choose any item in the **Name** column, and the **Request Headers** will show on the right. We can star the headers information into a dictionary named by **\"headers\"**, and call **\"headers = headers\"** in the `request.get()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3656cf4-1d0b-4aa8-a8af-fd0abdee6689",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://finance.yahoo.com/topic/stock-market-news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba9f69-8ae3-47a4-a97e-cad46dc078eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'accept-ranges': 'bytes', \n",
    "           'access-control-allow-origin': '*', \n",
    "           \"content-encoding\": \"gzip\", \n",
    "          \"cache-control\": \"public, max-age=86400\", \n",
    "          \"control-type\": \"text/javascript\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed246243-ee3e-4444-a7ca-9a20d818d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f0d5a-8969-4b71-a8ca-235fe3210568",
   "metadata": {},
   "source": [
    "#### 4.2 Use time to sleep to stop requesting too freaquently\n",
    "When web scraping the links from a website, I have used the list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7884b-39c6-4c87-a7ad-9f69028704fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [get_info(url) for url in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7ede7-ab1c-4d04-8b7f-bcc7ad247475",
   "metadata": {},
   "source": [
    "The list comprehension, even though saves some space for coding, will send three consecutive requests in a very short time. It won't be a big problem if there are only 3 requests but this becomes very problematic if the number increases significantly to like three hundred instead. Here, we need to adjust this step and insert a time sleep function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d88e89-a678-4cc1-b2eb-e22145d913e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for url in links:\n",
    "    #in case some page wouldn't work \n",
    "    try:\n",
    "        scraped = get_info(url)\n",
    "    except:\n",
    "        print('blocked')\n",
    "    ## adding time sleep will decrease the chance of being blocked, but increase operation time\n",
    "    time.sleep(1)\n",
    "    info.append(scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b073f-dbc9-41fe-8f28-b08b6514586d",
   "metadata": {},
   "source": [
    "Time.sleep will make your computer stop for a certain period before working on the next step. You can specify the waiting time to balance between avoiding be blocked and increasing operation time. Not that I also use try except function here, in case there are any problems during scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccaafb4-b3ff-4aa4-b5ce-29766f80ef4b",
   "metadata": {},
   "source": [
    "#### 4.3 Do not repeat the request for different information\n",
    "In the previous example, I wanted to extract the news body, date and headline from one website. Instead of requesting the website 3 times, I request it once, store the results and parse the response in soup, and extract three different peices of information from soup. Thus, we only request one time, instead of 3 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa82cefd-9363-4030-95e1-fd4c51269e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
